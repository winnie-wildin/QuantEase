"""Test complete baseline generation workflow"""
from app.database import SessionLocal
from app.models import Experiment, DatasetSample, ModelVariant, GeneratedOutput
from app.tasks.baseline_generation import generate_baseline_outputs
from sqlalchemy import text
from dotenv import load_dotenv
load_dotenv()
import os
print("ğŸ§ª Testing Baseline Generation Workflow...\n")

db = SessionLocal()

try:
    # Create test model
    print("1ï¸âƒ£ Creating test model...")
    db.execute(text("INSERT INTO models (name, description) VALUES ('test-model', 'Test')"))
    db.commit()
    model_id = db.execute(text("SELECT id FROM models WHERE name='test-model'")).fetchone()[0]
    print(f"   âœ… Model ID: {model_id}")
    
    # Create experiment
    print("\n2ï¸âƒ£ Creating experiment...")
    exp = Experiment(
        name="baseline_test",
        baseline_model_id=model_id,
        has_ground_truth=False,
        sample_count=3,
        status="created"
    )
    db.add(exp)
    db.commit()
    print(f"   âœ… Experiment ID: {exp.id}")
    
    # Create sample inputs
    print("\n3ï¸âƒ£ Creating test samples...")
    samples = [
        DatasetSample(experiment_id=exp.id, input_text="What is artificial intelligence?", position=0),
        DatasetSample(experiment_id=exp.id, input_text="What is machine learning?", position=1),
        DatasetSample(experiment_id=exp.id, input_text="What is deep learning?", position=2),
    ]
    db.add_all(samples)
    db.commit()
    print(f"   âœ… Created {len(samples)} samples")
    
    # Create baseline variant
    print("\n4ï¸âƒ£ Creating baseline variant...")
    variant = ModelVariant(
        experiment_id=exp.id,
        variant_type="baseline",
        model_name="llama-3.3-70b-versatile",
        inference_provider="groq",
        status="pending"
    )
    db.add(variant)
    db.commit()
    print(f"   âœ… Variant ID: {variant.id}")
    
    # Generate baseline outputs
    print("\n5ï¸âƒ£ Generating baseline outputs (this will take ~5-10 seconds)...")
    print("   " + "-"*50)
    
    result = generate_baseline_outputs(exp.id, variant.id)
    
    print("   " + "-"*50)
    print(f"   âœ… Generation complete!")
    print(f"   âœ… Status: {result['status']}")
    print(f"   âœ… Successful: {result['successful']}/{result['total_samples']}")
    if result['failed'] > 0:
        print(f"   âš ï¸  Failed: {result['failed']}")
    
    # Verify outputs
    print("\n6ï¸âƒ£ Verifying generated outputs...")
    db.refresh(variant)
    outputs = db.query(GeneratedOutput).filter(GeneratedOutput.variant_id == variant.id).all()
    
    print(f"   âœ… Found {len(outputs)} outputs")
    print(f"   âœ… Variant status: {variant.status}")
    print(f"   âœ… Variant progress: {variant.progress * 100:.0f}%")
    
    print("\n7ï¸âƒ£ Sample outputs:")
    successful_outputs = [o for o in outputs if o.is_successful]
    failed_outputs = [o for o in outputs if not o.is_successful]
    
    print(f"\n   âœ… Successful outputs: {len(successful_outputs)}")
    print(f"   âŒ Failed outputs: {len(failed_outputs)}")
    
    for i, output in enumerate(successful_outputs, 1):
        print(f"\n   Sample {i}:")
        print(f"   ğŸ“¥ Input: {output.sample.input_text}")
        print(f"   ğŸ“¤ Output: {output.output_text[:100]}...")
        print(f"   â±ï¸  Latency: {output.latency_ms:.2f}ms" if output.latency_ms else "   â±ï¸  Latency: N/A")
        print(f"   ğŸ”¢ Tokens: {output.token_count}" if output.token_count else "   ğŸ”¢ Tokens: N/A")
        print(f"   ğŸš€ Speed: {output.tokens_per_second:.2f} tok/s" if output.tokens_per_second else "   ğŸš€ Speed: N/A")
        print(f"   âœ… Success: Yes")
    
    if failed_outputs:
        print(f"\n   âŒ Failed samples:")
        for i, output in enumerate(failed_outputs, 1):
            print(f"      {i}. {output.sample.input_text} - Error: {output.generation_error}")
    
    # Calculate average metrics (only from successful outputs)
    print("\n8ï¸âƒ£ Performance metrics:")
    if successful_outputs:
        avg_latency = sum(o.latency_ms for o in successful_outputs if o.latency_ms) / len(successful_outputs)
        avg_tokens = sum(o.token_count for o in successful_outputs if o.token_count) / len(successful_outputs)
        avg_speed = sum(o.tokens_per_second for o in successful_outputs if o.tokens_per_second) / len(successful_outputs)
        
        print(f"   ğŸ“Š Average latency: {avg_latency:.2f}ms")
        print(f"   ğŸ“Š Average tokens: {avg_tokens:.1f}")
        print(f"   ğŸ“Š Average speed: {avg_speed:.2f} tokens/sec")
    else:
        print("   âš ï¸  No successful outputs to calculate metrics")
    
    # Cleanup
    print("\n9ï¸âƒ£ Cleaning up...")
    for output in outputs:
        db.delete(output)
    db.delete(variant)
    for sample in samples:
        db.delete(sample)
    db.delete(exp)
    db.commit()  # Commit BEFORE deleting model
    db.execute(text("DELETE FROM models WHERE name='test-model'"))
    db.commit()
    print("   âœ… Cleanup complete")
    
    print("\n" + "="*60)
    print("ğŸ‰ BASELINE GENERATION TEST COMPLETE!")
    print("="*60)
    print("\nâœ… Groq API integration working!")
    print("âœ… Baseline generation task working!")
    print("âœ… Outputs stored in database!")
    print("âœ… Progress tracking working!")
    print("âœ… Performance metrics recorded!")
    print("\nğŸŠ Phase 2 Successfully Completed!")
    print("\nğŸ“‹ What you accomplished:")
    print("   â€¢ Groq API client wrapper")
    print("   â€¢ Celery task for async generation")
    print("   â€¢ Real-time progress tracking")
    print("   â€¢ Database storage of outputs")
    print("   â€¢ Performance metrics collection")
    print("\nğŸš€ Ready for Phase 3: GGUF Model Loading!")
    
except Exception as e:
    print(f"\nâŒ Error: {e}")
    import traceback
    traceback.print_exc()
    db.rollback()
finally:
    db.close()